{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Mixture models\n",
    "In this homework, we will be working with The **<span style='background :greenyellow'>Federalist Papers</span>**, a collection of 85 articles written anonymously, under the collective pseudonym \"Publius\". Regardless of the anonymous authorship, we *do* know that three men were responsible for writing these papers: (1) Alexander Hamilton, (2) James Madison and (3) John Jay. Insofar as who wrote which article, it is unknown for the most part. Further, some articles are thought to be products of collaboration between multiple authors. \n",
    "\n",
    "Your task will be to attribute authorship to the 85 Federalist Papers through clustering. To accomplish this, we will model the 85 Federalist Papers as word counts, following Poisson distributions. You will implement an **<span style='background :greenyellow'>Expectation-Maximization (EM) algorithm</span>** to find optimal clusters.\n",
    "\n",
    "Once the model is trained, we will use articles with known authors to assess our clusters. Specifically, we will perform a **<span style='background :greenyellow'>hypergeometric test</span>** (a.k.a. Fischer's one-sided exact test) that will tell us how likely a given cluster was authored by each of the three authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# If any of these do not import, make sure to install them \n",
    "# ... with your package manager of choice!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import scipy\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import hypergeom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data featurization\n",
    "We will use counts of **<span style='background :greenyellow'>function words</span>** as features for clustering the articles. Function words can be thought of as the filler words of a language, and therefore the most abundant in any given text sample. They are often prepositions and pronouns, such as \"the\", \"these\", \"for\", \"at\", and \"you\".\n",
    "\n",
    "Arguably, one may ascertain writing style (and hence, authorship) from looking at only the counts of function words, ignoring all other non-function (e.g. \"content\") words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 85 samples and 8612 unique words in the Federalist Papers.\n",
      "There are 199 unique function words in the Federalist Papers.\n",
      "\n",
      "The dataset has 85 samples and 199 features.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with gzip.open(\"preprocessed_documents.pgz\",\"rb\") as f:\n",
    "    documents, counter = pd.read_pickle(f)\n",
    "    \n",
    "dataMat = scipy.sparse.vstack([doc['acounts'] for doc in documents])\n",
    "dataMat = np.asarray( dataMat.todense().astype('int32') )\n",
    "print(f\"There are {dataMat.shape[0]} samples and {dataMat.shape[1]} \\\n",
    "unique words in the Federalist Papers.\")\n",
    "\n",
    "# Load function word list.\n",
    "with codecs.open('updated_functionWords.txt') as f:\n",
    "    func_words = f.read().splitlines() \n",
    "    \n",
    "# Get counts of function words that occur in the Papers.\n",
    "func_idc = np.nonzero(np.in1d(counter.get_feature_names(),(func_words)))[0]\n",
    "selected_words = np.array(counter.get_feature_names())[func_idc]\n",
    "\n",
    "print( f\"There are {len(func_idc)} unique function words \\\n",
    "in the Federalist Papers.\")\n",
    "# print( f\"{selected_words}\")\n",
    "\n",
    "dataMat_selected = np.asarray(dataMat[:,func_idc])\n",
    "\n",
    "print(f\"\\nThe dataset has {dataMat_selected.shape[0]} samples \\\n",
    "and {dataMat_selected.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Model definition: Mixture of Poissons\n",
    "\n",
    "Remember, the features of our data are <span style='background :greenyellow'>word counts</span>, or frequences.\n",
    "\n",
    "The **<span style='background :greenyellow'>Poisson distribution</span>** is a commonly encounted distribution used for modeling counts of events. So it is natural for us to assume that our word counts follow a Poisson distribution, where our \"events\" are word usage instances.\n",
    "\n",
    "Furthermore, we will assume that each article belongs to a single cluster. There are <span style='background :greenyellow'>four clusters</span>, one associated with each of the authors of the Federalist Papers and one \"disputed author\" cluster. \n",
    "\n",
    "Each cluster will have a <span style='background :greenyellow'>specific Poisson distribution associated with *each* individual word</span>.\n",
    "\n",
    "The \"work\" part of this homework will be deriving the formulation of the mixture of Poissons model and fitting it to the using the EM algorithm data. \n",
    "\n",
    "\n",
    "### Mathematical details\n",
    "\n",
    "The Poisson probability mass function (PMF) is defined as: \n",
    "\n",
    "$$p(k \\mid \\lambda) = \\frac{\\lambda^{k} e^{-\\lambda}}{k!} \\tag{1}$$ \n",
    "\n",
    "where $k$ is the count and $\\lambda$ is the mean rate of occurence.\n",
    "\n",
    "Notation for the model:\n",
    "\n",
    "- <span style='background :greenyellow'>$\\large x_i$</span> â€” feature vector of the $i^{th}$ sample\n",
    "    - Note that $x_{i, j}$ are the $j^{th}$ features of the $i^{th}$ sample. \n",
    "- <span style='background :greenyellow'>$\\large h_i$</span> â€” index of the cluster for the $i^{th}$ sample. \n",
    "- <span style='background :greenyellow'> $\\large \\lambda_m$</span> â€” lambda vector for the $m^{th}$ cluster\n",
    "    - Note that $\\lambda_{m,j}$ is the Possion PMF parameter for cluster $m$ and feature $j$.\n",
    "\n",
    "- <span style='background :greenyellow'>$\\large \\pi_c$</span> $= \\normalsize p(h_i = c)$ \n",
    "- $\\normalsize p(x_i \\mid h_i = m, \\lambda) = \\prod_j p(x_{i,j}\\mid \\lambda_{m,j})$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Likelihood expressions and lower bound derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q_1'></a>\n",
    "1) **<span style='background :yellow'>(1pt)</span>** <font color=blue>Write the expression for the log probability of sample $x_i$ given that it belongs to cluster $m$, (i.e. $h_i = m$). </font> Refer to the Poisson PMF in Eq. 1 above.\n",
    "<br><br>\n",
    "$$ \\large\n",
    "\\log p(x_i| h_i = m, \\lambda) = \\sum_j x_{i,j} \\log\\lambda_{m,j} -\\lambda_{m,j} - \\log x_{i,j}!\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **<span style='background :yellow'>(1pt)</span>** <font color=blue> In the cell below, replace the ellipsis (there's only one) with code to complete `logprobvec`, a function that computes log probability of a single sample, $\\log p(x_i \\mid \\lambda_m)$, which you defined in [Question 1](#Q_1) above.\n",
    "    \n",
    " Inputs:\n",
    "- `x`: feature vector for sample $x_i$. \n",
    "- `ls`: lambda vector.\n",
    "    \n",
    "Return values:\n",
    "- `lp`: resultant log-probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary log-domain functions.\n",
    "def logsum(lp):\n",
    "    m = np.max(lp)    \n",
    "    return np.log(np.sum(np.exp(lp-m))) + m\n",
    "def logfactorial(x):    \n",
    "    return np.sum(np.log(np.arange(1,x+1)))\n",
    "\n",
    "# Function to calculate log probability.\n",
    "def logprobvec(xs, ls): \n",
    "    logfactorial_val = np.zeros((len(xs, )))\n",
    "    lp = xs * np.log(ls) - ls - logfactorial_val\n",
    "    lp = np.sum(lp)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test? True\n"
     ]
    }
   ],
   "source": [
    "# Small test to check that function works.\n",
    "test1 = np.array([5, 33, 211, 474])\n",
    "test2 = np.array([4, 60, 300, 600])\n",
    "res = logprobvec(test1, test2)\n",
    "did_u_pass = np.allclose( res, 3413.687601 ) \n",
    "print(f\"Passed test? {did_u_pass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q_3'></a>\n",
    "3) **<span style='background :yellow'>(1pt)</span>** <font color=blue> Write the expression for the log marginal probability of sample $x_i$ in terms of $p(x_i\\mid\\lambda,h_i)$ and $\\pi$.\n",
    "<br><br>\n",
    "$$\\large\n",
    "\\log p(x_i \\mid \\lambda,\\pi) = \\log \\sum_c (\\pi_c * p(x_i\\mid\\lambda,h_i = c))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **<span style='background :yellow'>(1pt)</span>** <font color=blue> Write the expression for the model's log-likelihood. </font> This expression is over *all* the samples, so refer to the log probability of a *single* sample that you derived above in [Question 3](#Q_#). \n",
    "<font color=blue>\n",
    "<br><br>\n",
    "$$ \\large\n",
    "    \\begin{aligned}\n",
    "LL(\\lambda,\\pi) &= \\log( \\prod_{i} p(x_{i}))\\\\\n",
    "&= \\sum_{i} \\log \\sum_c (\\pi_c * p(x_i\\mid\\lambda,h_i = c)) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q_5'></a>\n",
    "5) **<span style='background :yellow'>(1pt)</span>** <font color=blue> Apply Jensen’s inequality to derive the expression for the lower-bound on log-likelihood.</font> *(Refer to Lec. 11)*\n",
    "<br><br>\n",
    "<font color=blue>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\lambda, \\pi) = \\log( \\prod_{i} p(x_{i})) &= \\sum_{i} \\log \\left \\{ \\sum_{c} q(h_i=c) \\frac{ p(x_{i}, h_i = c) }{q(h_i=c)} \\right \\} \\\\\n",
    "& \\ge \\sum_{i} \\sum_{c} \\log \\left\\{ \\frac{p(x_i, h_i = c)}{q(h_i = c)})\\right\\} \\\\\n",
    "&= \\sum_{i} \\sum_{c} \\log \\left\\{  p(x_i,h_i = c) \\right\\} - \\sum_{i} \\sum_{c}  \\log \\left\\{ q(h_i = c)\\right\\}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. E-step: Calculating model posterior log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q_6'></a>\n",
    "6) **<span style='background :yellow'>(1pt)</span>** If we let $q(h_i)$ be the posterior probability $p(h_i\\mid x_i,\\lambda, \\pi)$, then we obtain a tight lower-bound on the log-likelihood. <font color=blue> Use Bayes rule to derive the expression for the posterior probability that sample $i$ belongs to cluster $m$. </font>Your answer should include the variables $\\pi_c$ and $\\pi_m$.\n",
    "<br><br> <font color=blue>\n",
    "$$ \\large\n",
    "\\begin{aligned}\n",
    "p(h_i = m\\mid x_i, \\lambda, \\pi) &= \\frac{p(x_i | h_i = m) \\pi_m \\; }{\\sum_c p(x_i | h_i = c) \\pi_c\\;}\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q_7'></a>\n",
    "\n",
    "7) **<span style='background :yellow'>(1pt)</span>**  <font color=blue> In the cell below, replace the ellipses (2 of them) with code to complete `get_logposterior`, a function that computes the log posterior for all samples, $p(h = m\\mid x, \\lambda, \\pi)$.</font>\n",
    "    \n",
    "Here, K is the number of clusters (authors), F is the number of features (words), and N is the number of samples (Papers).\n",
    "    \n",
    "Inputs:\n",
    "- `X`: matrix of shape (N, F), the feature matrix. \n",
    "- `lambdas`: matrix of shape (K, F). \n",
    "- `pis`: vector of shape (K).\n",
    "    \n",
    "Return values:\n",
    "- `loglik`: log-likelihood of the model as you defined in [Question 5](#Q_5).\n",
    "- `logprobs`: matrix of shape (K, N), contains a log posterior probability (as derived in [Question 6](#Q_6)) for each cluster.\n",
    "- `labels`: vector of shape (N), contains the most probable cluster per each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logposterior(X,lambdas,pis):\n",
    "    K = lambdas.shape[0] # Number of clusters.    \n",
    "    F = lambdas.shape[1] # Number of features.\n",
    "    N = X.shape[0]       # Number of samples (Papers).\n",
    "    loglik = 0\n",
    "    logprobs = np.zeros((K,N))\n",
    "    labels = np.zeros(N)\n",
    "    for i in range(N) :\n",
    "        x_i = X[i,:]\n",
    "        for k in range(K):\n",
    "            ls = lambdas[k,:]\n",
    "            logprobs[k,i] = logprobvec(x_i, ls) + np.log(pis[k])               ## FILL-IN-THE-BLANK ##   \n",
    "        docloglik = logsum(logprobs[:,i])    \n",
    "        loglik = loglik + docloglik\n",
    "        logprobs[:,i] = logprobs[:,i] - docloglik                  ## FILL-IN-THE-BLANK ##  \n",
    "        labels[i] = np.argmax(logprobs[:,i])\n",
    "    return logprobs, loglik, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test? True\n"
     ]
    }
   ],
   "source": [
    "# Small test to check that function works.\n",
    "test_x = np.array( [[7, 4, 6], [2, 8, 1], [3, 3, 9]], dtype='float32' )\n",
    "test_l = np.tile( np.round(np.mean(test_x, axis=0)), (3,1))\n",
    "np.random.seed(10)\n",
    "test_l = test_l + 1 + np.abs(np.random.randn(test_l.shape[0], test_l.shape[1]))*2\n",
    "test_pis = np.array([0.33, 0.33, 0.33])\n",
    "res_lp, res_ll, res_lab = get_logposterior( test_x, test_l, test_pis )\n",
    "check_1 = np.allclose( res_lp, [[-1.57270506, -4.35048081, -2.08895623],\n",
    "                                   [-1.3579241 , -1.1180958 , -0.75511075],\n",
    "                                   [-0.62488554, -0.41521594, -0.90084777]] )  \n",
    "check_2 = np.allclose( res_ll, 21.969895 ) \n",
    "check_3 = np.allclose( res_lab, [2, 2, 1])\n",
    "did_u_pass = (sum([check_1, check_2, check_3])==3)\n",
    "\n",
    "print(f\"Passed test? {did_u_pass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. M-step: Updating model parameters ($\\lambda$ and $\\pi$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **<span style='background :yellow'>(2pt)</span>** <font color=blue> Derive the update function for $\\lambda_{m, j}$. To do this, take the derivative of the lower-bound on log-likelihood w.r.t. $\\lambda_{m, j}$, set it to 0, and solve for $\\lambda_{m, j}$. </font> (Reminder: $\\lambda_{m}$ is the vector of lambdas describing the $m$th cluster. And $\\lambda_{m, j}$ is a scalar value that describes the distribution of the $j$th word for the $m$th cluster.)\n",
    "<br> <br>    \n",
    "- <font color=blue> Below, the steps required for the derivation are indicated, but you need to complete them by filling in the ellipses. \n",
    "<br> <br> \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i} \\sum_{j} \\sum_{m} q(h_i=m)  \\log \\left \\{  p(x_{i,j}, h_i = m) \\right \\}\n",
    "&=\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i}  q(h_i=m) \\log \\left \\{  p(x_{i,j} | h_i = m)p(h_i = m)  \\right \\} \\\\\n",
    "&=\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i}  q(h_i=m) (\\log \\left \\{  p(x_{i,j} | h_i = m \\right \\} + \\log p(h_i = m) ) \\\\\n",
    "&= \\sum_{i}  q(h_i=m) \\frac{\\partial \\log \\left \\{  \\frac{\\lambda_{m_j}^{x_{i,j}} e^{-\\lambda_{m_j}}}{x_{i,j}! } \\right \\}  + \\log(\\pi_m) } {\\partial \\lambda_{m,j}} \\\\\n",
    "&= \\sum_{i}  q(h_i=m) \\frac{\\partial (  x_{i,j} * \\log(\\lambda_{m_j}) - \\lambda_{m_j} - log(x_{i,j}!)\n",
    "+ log(\\pi_m))} {\\partial \\lambda_{m,j} } \\\\\n",
    "&= \\sum_{i}  q(h_i=m) \\left( \\frac{ x_{i,j} }{\\lambda_{m_j}} - 1 \\right)\n",
    "\\end{aligned}\n",
    "$$ \n",
    "<br> <br>\n",
    "- <font color=blue> After you find the derivative, set it to 0, and solve for $\\lambda_{m, j}$.\n",
    "<br> <br>\n",
    "$$ \\large\n",
    "\\lambda_{m, j} = \\frac{ \\sum_i q(h_i=m) (\\frac{x_{i,j}} {\\lambda_{m_j}} - 1) }{\\sum_i q(h_i=m) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) **<span style='background :yellow'>(1pt)</span>**  <font color=blue> Derive the update formulation for $\\pi_m$. </font> (Reminder: $\\pi_{m}$ is a scalar value describing the prior probability of the $m$th cluster.) Below, we provide you with the derivative of the lower-bound (Langrangian, defined below) w.r.t $\\pi_m$ and w.r.t $\\gamma$. <font color=blue>  Using these equalities, solve for $\\pi_m$. </font>\n",
    "<br><br> \n",
    "$$ \\scriptsize\n",
    "    \\text{Lagrangian}\\> L(\\pi, \\gamma) = \\sum_{i}  q(h_i=m) \\log \\left\\{ \\pi_m \\right\\} + \\gamma(\\sum_c \\pi_c - 1 )\n",
    "$$ <br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\sum_i \\frac{q(h_i=m)}{\\pi_m} + \\gamma = 0 \\\\\n",
    "    &\\sum_c \\pi_c= 1\n",
    "\\end{aligned}\n",
    "$$ \n",
    "Using the above two equalities, we have <font color=blue>\n",
    "<br><br> $$ \\large\n",
    "\\pi_m = -\\frac{N}{\\gamma}\n",
    "$$ <br><br> </font>\n",
    "where $N$ is the number of samples ($\\leftarrow$ This is like a hint. Your answer should include \"$N$\" somewhere.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **<span style='background :yellow'>(1pt)</span>**  <font color=blue>  In the cell below, replace the ellipses (4 of them) with code to complete `update_params`, a function that updates the model parameters, $\\lambda_{m,j}$ and $\\pi_m$.\n",
    "   \n",
    "Here, K is the number of clusters (authors), F is the number of features (words), and N is the number of samples (Papers).\n",
    "        \n",
    "Inputs:\n",
    "- `qs`: matrix of shape (K, N), the prior probabilities of a sample belonging to each cluster. \n",
    "- `X`: matrix of shape (N, F), the data. \n",
    "    \n",
    "Return values:\n",
    "- `ls`: array of shape (K, F), the updated lambdas.\n",
    "- `pis`: vector of shape (K), the updated (posterior) probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(qs,xs):\n",
    "    suff = np.dot(qs,xs)            ## FILL-IN-THE-BLANK ## \n",
    "    ls = (suff+0.001)/(0.001+np.sum(qs,1)[:,np.newaxis]) # Add small constant.\n",
    "    pis = np.ones(qs.shape[0])      ## FILL-IN-THE-BLANK ## \n",
    "    pis = alpha = np.sum(qs, axis=1)/qs.shape[1] ## FILL-IN-THE-BLANK ## \n",
    "    return ls, pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-454fa9724ccc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Small test to check that function works.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mres_ls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_pis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_qs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Small test to check that function works.\n",
    "test_qs = np.array([[.9, .3], [.05, .4], [.05, .3]])\n",
    "test_xs = np.array( [[7, 4, 6], [2, 8, 1]], dtype='float32' )\n",
    "res_ls, res_pis = update_params(test_qs, test_xs)\n",
    "\n",
    "check_1 = np.allclose( res_ls,[[ 5.746044,  4.996669,  4.74687 ],\n",
    "                               [ 2.552106, 7.541019,  1.554323],\n",
    "                               [ 2.709401,  7.410256,  1.712250]] )  \n",
    "check_2 = np.allclose( res_pis, [0.6, 0.225, 0.175] )\n",
    "did_u_pass = (sum([check_1, check_2])==2)\n",
    "\n",
    "print(res_ls)\n",
    "print(res_pis)\n",
    "\n",
    "print(f\"Passed test? {did_u_pass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "11) **<span style='background :yellow'>(1pt)</span>**  <font color=blue> Run the code below, which will invoke your E-step (```get_logposterior```) and M-step (```update_params```). Make sure that you print out the \"Best\" results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def report_labels(labels,side_info):\n",
    "    for l in np.unique(labels):\n",
    "        print('CLUSTER id: {0:d}'.format(int(l)))\n",
    "        members = np.nonzero(labels==l)\n",
    "        tmp = Counter( np.sort(np.array(side_info)[members]) )\n",
    "        report = []\n",
    "        for author in set(side_info):\n",
    "            if author not in tmp.keys():\n",
    "                report.append([author,0])\n",
    "            elif author == '':\n",
    "                report.append(['DISPUTED',tmp[author]])\n",
    "            else:\n",
    "                report.append([author,tmp[author]])\n",
    "        for author,cnt in report:\n",
    "            s = f\"â€¢ Papers by {author}:\"\n",
    "            print(f\"{s:22} {cnt}\")\n",
    "            \n",
    "def fit(X,K,side_info):    \n",
    "    L = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    best_loglik = -1e+308\n",
    "    best_logliks, best_labels, best_ls, best_pis = [], [], [], []\n",
    "    for s in range(100): # Run EM algorithm 100 times\n",
    "        np.random.seed(s)\n",
    "        ls = np.mean(X,0)*(1.0 + 0.5*(np.random.rand(K,L)-0.5))\n",
    "        pis = [1./K]*K\n",
    "        logliks = []\n",
    "        for it in range(50): # Each time run for 50 iterations\n",
    "            logqs, loglik, labels = get_logposterior(X,ls,pis)\n",
    "            qs = np.exp(logqs)\n",
    "            logliks.append(loglik)\n",
    "            ls,pis = update_params(qs,X)\n",
    "        if loglik > best_loglik:\n",
    "            best_loglik = loglik    \n",
    "            print( f'\\nA fit with better log-likelihood ({loglik}) found for for seed {s}.')\n",
    "#             report_labels(labels,side_info)\n",
    "            best_ls, best_pis, best_labels, best_logliks = ls, pis, labels, logliks         \n",
    "    print('\\nBest:')\n",
    "    report_labels(best_labels,side_info)\n",
    "    return best_ls, best_pis, best_labels, best_logliks\n",
    "\n",
    "ls_fit, pis_fit, labels_fit, logliks_fit = fit(dataMat_selected, 4, \n",
    "                                               [doc['authors'] for doc in documents])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) **<span style='background :yellow'>(1pt)</span>** <font color=blue> Run the cell below to plot the log-likelihood trend across 50 EM iterations. </font>You should see your log-likelihoods increase logarithmically. If they don't, well, that sucks; you need to debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logliks_fit)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log-likelihood\")\n",
    "plt.title(\"Log-Likelihood trend (50 EM iterations)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Evaluate authorship via Hypergeometric test\n",
    "\n",
    "\n",
    "Lastly, we will perform a hypergeometric test that will assess our clusters for enrichment in articles authored by each of the people of interest (Hamilton, Madison, Jay).\n",
    "\n",
    "The <span style='background :greenyellow'>null hypothesis</span> is that the assignment of articles to clusters is completely random, and there is no association between our clusters and authors of articles. \n",
    "\n",
    "\n",
    "Given a cluster and an author, the p-values provided by the hypergeometric test correspond to the  <span style='background :greenyellow'> chance that the author was assigned to the articles in the cluster ***at random***.</span> Lower p-value confers *higher significance*. Therefore, if we observe a low p-value then we can reject the null in favor of the alternative hypothesis, which is: the association between the given author and the given cluster is NOT by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_table( labels, documents ):\n",
    "    K = len(np.unique(labels))\n",
    "    authorList = np.asarray([doc['authors'] for doc in documents])\n",
    "    nameList = np.asarray( ['JAY', 'MADISON', 'HAMILTON'] )\n",
    "    nameList2 = np.asarray( ['JAY', 'MADISON', 'HAMILTON', ''] )\n",
    "    nameNum = {'JAY': 5, 'MADISON': 14, 'HAMILTON': 51}\n",
    "    enrichment = np.zeros((K, 3))\n",
    "    numberTab = np.zeros((K,4))\n",
    "    for i in np.arange(K):\n",
    "        cList = authorList[np.nonzero(labels==i)[0]]\n",
    "        cnt = 0\n",
    "        for j in nameList:\n",
    "            rv = hypergeom(70, nameNum[j], len(cList) )\n",
    "            cIntersect = len(np.nonzero(cList==j)[0])\n",
    "            enrichment[i, cnt] = 1-rv.cdf(cIntersect)\n",
    "            numberTab[i, cnt] = cIntersect\n",
    "            cnt = cnt + 1\n",
    "        cnt = 0\n",
    "        for j in nameList2:\n",
    "            cIntersect = len(np.nonzero(cList==j)[0])\n",
    "            numberTab[i, cnt] = cIntersect\n",
    "            cnt = cnt + 1\n",
    "    return enrichment, numberTab\n",
    "\n",
    "def report_table(enrichment, numberTab):\n",
    "    nameList = np.asarray( ['JAY', 'MADISON', 'HAMILTON'] )\n",
    "    nameList2 = np.asarray( ['JAY', 'MADISON', 'HAMILTON', 'DISPUTED'] )\n",
    "    print( '* * * HYPERGEOMETRIC TEST RESULTS * * * \\n')\n",
    "    print( 'p-values: ')\n",
    "    print('\\t\\t', end=\"\")\n",
    "    for z in nameList:\n",
    "        print('{0:<8}\\t'.format(z), end='')\n",
    "    print()\n",
    "    for i in range(4):\n",
    "        print('cluster {}\\t'.format(i), end='')\n",
    "        for j in range(3):\n",
    "            print('{0:3f}\\t'.format(enrichment[i, j]), end='')\n",
    "        print()\n",
    "\n",
    "    print( '\\n\\ncount table: ')\n",
    "    print('\\t\\t', end=\"\")\n",
    "    for z in nameList2:\n",
    "        print('{0:<8}\\t'.format(z), end='')\n",
    "    print()\n",
    "    for i in range(4):\n",
    "        print('cluster {}\\t'.format(i), end='')\n",
    "        for j in range(4):\n",
    "            print('{0:<8}\\t'.format(int(numberTab[i, j])), end='')\n",
    "        print()\n",
    "        \n",
    "enrichment, numberTab = gen_table(labels_fit, documents)\n",
    "report_table(enrichment,numberTab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) **<span style='background :yellow'>(2pt)</span>**  <font color=blue>According to the results of the hypergeometric test, which **single** author is *most likely* to have penned disputed articles? Provide a *short* (**no more than 50 words**) explanation; reference why the p-value and the respective counts lead you to your conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
